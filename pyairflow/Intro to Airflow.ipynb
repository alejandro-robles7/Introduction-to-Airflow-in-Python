{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Intro to Airflow\n",
    "An introduction to the components of Apache Airflow and why to use them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running a task in Airflow\n",
    "You've just started looking at using Airflow within your company and would like to try to run a task within the Airflow platform. You remember that you can use the airflow run command to execute a specific task within a workflow. Note that an error while using airflow run will return airflow.exceptions.AirflowException: on the last line of output.\n",
    "\n",
    "An Airflow DAG is set up for you with a dag_id of etl_pipeline. The task_id is download_file and the start_date is 2020-01-08. All other components needed are defined for you.\n",
    "\n",
    "Which command would you enter in the console to run the desired task?\n",
    "\n",
    "### airflow run etl_pipeline download_file 2020-01-08"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examining Airflow commands\n",
    "While researching how to use Airflow, you start to wonder about the airflow command in general. You realize that by simply running airflow you can get further information about various sub-commands that are available.\n",
    "\n",
    "Which of the following is NOT an Airflow sub-command?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: airflow [-h]\r\n",
      "               {backfill,list_dag_runs,list_tasks,clear,pause,unpause,trigger_dag,delete_dag,show_dag,pool,variables,kerberos,render,run,initdb,list_dags,dag_state,task_failed_deps,task_state,serve_logs,test,webserver,resetdb,upgradedb,checkdb,shell,scheduler,worker,flower,version,connections,create_user,delete_user,list_users,sync_perm,next_execution,rotate_fernet_key,config,info}\r\n",
      "               ...\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  {backfill,list_dag_runs,list_tasks,clear,pause,unpause,trigger_dag,delete_dag,show_dag,pool,variables,kerberos,render,run,initdb,list_dags,dag_state,task_failed_deps,task_state,serve_logs,test,webserver,resetdb,upgradedb,checkdb,shell,scheduler,worker,flower,version,connections,create_user,delete_user,list_users,sync_perm,next_execution,rotate_fernet_key,config,info}\r\n",
      "                        sub-command help\r\n",
      "    backfill            Run subsections of a DAG for a specified date range.\r\n",
      "                        If reset_dag_run option is used, backfill will first\r\n",
      "                        prompt users whether airflow should clear all the\r\n",
      "                        previous dag_run and task_instances within the\r\n",
      "                        backfill date range. If rerun_failed_tasks is used,\r\n",
      "                        backfill will auto re-run the previous failed task\r\n",
      "                        instances within the backfill date range.\r\n",
      "    list_dag_runs       List dag runs given a DAG id. If state option is\r\n",
      "                        given, it will onlysearch for all the dagruns with the\r\n",
      "                        given state. If no_backfill option is given, it will\r\n",
      "                        filter outall backfill dagruns for given dag id.\r\n",
      "    list_tasks          List the tasks within a DAG\r\n",
      "    clear               Clear a set of task instance, as if they never ran\r\n",
      "    pause               Pause a DAG\r\n",
      "    unpause             Resume a paused DAG\r\n",
      "    trigger_dag         Trigger a DAG run\r\n",
      "    delete_dag          Delete all DB records related to the specified DAG\r\n",
      "    show_dag            Displays DAG's tasks with their dependencies\r\n",
      "    pool                CRUD operations on pools\r\n",
      "    variables           CRUD operations on variables\r\n",
      "    kerberos            Start a kerberos ticket renewer\r\n",
      "    render              Render a task instance's template(s)\r\n",
      "    run                 Run a single task instance\r\n",
      "    initdb              Initialize the metadata database\r\n",
      "    list_dags           List all the DAGs\r\n",
      "    dag_state           Get the status of a dag run\r\n",
      "    task_failed_deps    Returns the unmet dependencies for a task instance\r\n",
      "                        from the perspective of the scheduler. In other words,\r\n",
      "                        why a task instance doesn't get scheduled and then\r\n",
      "                        queued by the scheduler, and then run by an executor).\r\n",
      "    task_state          Get the status of a task instance\r\n",
      "    serve_logs          Serve logs generate by worker\r\n",
      "    test                Test a task instance. This will run a task without\r\n",
      "                        checking for dependencies or recording its state in\r\n",
      "                        the database.\r\n",
      "    webserver           Start a Airflow webserver instance\r\n",
      "    resetdb             Burn down and rebuild the metadata database\r\n",
      "    upgradedb           Upgrade the metadata database to latest version\r\n",
      "    checkdb             Check if the database can be reached.\r\n",
      "    shell               Runs a shell to access the database\r\n",
      "    scheduler           Start a scheduler instance\r\n",
      "    worker              Start a Celery worker node\r\n",
      "    flower              Start a Celery Flower\r\n",
      "    version             Show the version\r\n",
      "    connections         List/Add/Delete connections\r\n",
      "    create_user         Create an account for the Web UI (FAB-based)\r\n",
      "    delete_user         Delete an account for the Web UI\r\n",
      "    list_users          List accounts for the Web UI\r\n",
      "    sync_perm           Update permissions for existing roles and DAGs.\r\n",
      "    next_execution      Get the next execution datetime of a DAG.\r\n",
      "    rotate_fernet_key   Rotate all encrypted connection credentials and\r\n",
      "                        variables; see\r\n",
      "                        https://airflow.readthedocs.io/en/stable/howto/secure-\r\n",
      "                        connections.html#rotating-encryption-keys.\r\n",
      "    config              Show current application configuration\r\n",
      "    info                Show information about current Airflow and environment\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "\r\n",
      "airflow command error: the following arguments are required: subcommand, see help above.\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "! airflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Answer: edit-dag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining a simple DAG\n",
    "You've spent some time reviewing the Airflow components and are interested in testing out your own workflows. To start you decide to define the default arguments and create a DAG object for your workflow."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Define the default_args dictionary\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2020, 1, 14),\n",
    "  'retries': 2\n",
    "}\n",
    "\n",
    "# Instantiate the DAG object\n",
    "etl_dag = DAG('example_etl', default_args=default_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with DAGs and the Airflow shell\n",
    "While working with Airflow, sometimes it can be tricky to remember what DAGs are defined and what they do. You want to gain some further knowledge of the Airflow shell command so you'd like to see what options are available.\n",
    "\n",
    "Command\n",
    "``` bash\n",
    "airflow list_dags\n",
    "```\n",
    "\n",
    "There are two DAGs, example_dag and update_state. Using the Airflow shell command can provide a lot of useful information when creating and troubleshooting workflows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Starting the Airflow webserver\n",
    "You've successfully created some DAGs within Airflow using the command-line tools, but notice that it can be a bit tricky to handle scheduling / troubleshooting / etc. After reading the documentation further, you realize that you'd like to access the Airflow web interface. For security reasons, you'd like to start the webserver on port 9090.\n",
    "\n",
    "Which airflow command would you use to start the webserver on port 9090?\n",
    "\n",
    "Airflow is installed and accessible from the command line. Remember to use the airflow -h command if needed. airflow <subcommand> -h will provide further detail.\n",
    "\n",
    "```bash\n",
    "airflow webserver -p 9090\n",
    "```\n",
    "Sometimes the defaults for Airflow aren't exactly what you'd like to use. Using the built in tools to configure the setup to your specifications is a very common function of a data engineer.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examining DAGs with the Airflow UI\n",
    "You've become familiar with the basics of an Airflow DAG and the basics of interacting with Airflow on the command-line. Your boss would like you to show others on your team how to examine any available DAGs. In this instance, she would like to know which operator is NOT in use with the DAG called update_state, as your team is trying to verify the components used in production workflows.\n",
    "\n",
    "Remember that the Airflow UI allows various methods to view the state of DAGs. The Tree View lists the tasks and any ordering between them in a tree structure, with the ability to compress / expand the nodes. The Graph View shows any tasks and their dependencies in a graph structure, along with the ability to access further details about task runs. The Code view provides full access to the Python code that makes up the DAG.\n",
    "\n",
    "Remember to select the operator NOT used in this DAG.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Checking missing operator](images/chap1_1.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}