{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Implementing Airflow DAGs\n",
    "Learn the basics of implementing Airflow DAGs using operators, tasks, and scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining a BashOperator task\n",
    "The BashOperator allows you to specify any given Shell command or script and add it to an Airflow workflow. This can be a great start to implementing Airflow in your environment.\n",
    "\n",
    "As such, you've been running some scripts manually to clean data (using a script called cleanup.sh) prior to delivery to your colleagues in the Data Analytics group. As you get more of these tasks assigned, you've realized it's becoming difficult to keep up with running everything manually, much less dealing with errors or retries. You'd like to implement a simple script as an Airflow operator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Define the default_args dictionary\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2020, 1, 14),\n",
    "  'retries': 2\n",
    "}\n",
    "\n",
    "# Instantiate the DAG object\n",
    "analytics_dag = DAG('example_etl', default_args=default_args)\n",
    "\n",
    "# Define the BashOperator\n",
    "cleanup = BashOperator(\n",
    "    task_id='cleanup_task',\n",
    "    # Define the bash_command\n",
    "    bash_command='cleanup.sh',\n",
    "    # Add the task to the dag\n",
    "    dag=analytics_dag\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " You've created your first operator to run a predefined Bash script. This is a common method to migrate from standalone scripts to an Airflow based workflow."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple BashOperators\n",
    "Airflow DAGs can contain many operators, each performing their defined tasks.\n",
    "\n",
    "You've successfully implemented one of your scripts as an Airflow task and have decided to continue migrating your individual scripts to a full Airflow DAG. You now want to add more components to the workflow. In addition to the cleanup.sh used in the previous exercise you have two more scripts, consolidate_data.sh and push_data.sh. These further process your data and copy to its final location."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define a second operator to run the `consolidate_data.sh` script\n",
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    bash_command='consolidate_data.sh',\n",
    "    dag=analytics_dag)\n",
    "\n",
    "# Define a final operator to execute the `push_data.sh` script\n",
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push_data.sh',\n",
    "    dag=analytics_dag)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You've correctly defined multiple BashOperators within an Airflow workflow. This adds reliability and repeatablity to common tasks run from the shell."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define order of BashOperators\n",
    "Now that you've learned about the bitshift operators, it's time to modify your workflow to include a pull step and to include the task ordering. You have three currently defined components, cleanup, consolidate, and push_data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<Task(BashOperator): pushdata_task>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a new pull_sales task\n",
    "pull_sales = BashOperator(\n",
    "    task_id='pullsales_task',\n",
    "    bash_command='wget https://salestracking/latestinfo?json',\n",
    "    dag=analytics_dag\n",
    ")\n",
    "\n",
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup\n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "consolidate << cleanup\n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You've correctly defined task dependencies within you workflow. This provides the ability to actually complete steps in the order that makes sense rather than writing the steps in your code. This is helpful for troubleshooting, logging, updates, etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Determining the order of tasks\n",
    "While looking through a colleague's workflow definition, you're trying to decipher exactly in which order the defined tasks run. The code in question shows the following:\n",
    "\n",
    "```python\n",
    "pull_data << initialize_process\n",
    "pull_data >> clean >> run_ml_pipeline\n",
    "generate_reports << run_ml_pipeline\n",
    "```\n",
    "\n",
    "1. initialize_process\n",
    "2. pull_data\n",
    "3. clean\n",
    "4. run_ml_pipeline\n",
    "5. generate_reports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Troubleshooting DAG dependencies\n",
    "You've created a DAG with intended dependencies based on your workflow but for some reason Airflow won't load / execute the DAG. Try using the terminal to:\n",
    "\n",
    "- List the DAGs.\n",
    "- Decipher the error message.\n",
    "- Use cat workspace/dags/codependent.py to view the Python code.\n",
    "- Determine which of the following lines should be removed from the Python code. You may want to consider the last line of the file.\n",
    "\n",
    "```bash\n",
    "airflow list_dags\n",
    "cat workspace/dags/codependent.py\n",
    "```\n",
    "\n",
    "Python Code\n",
    "```python\n",
    "# task1 must run before task2 which must run before task3\n",
    "task1 >> task2\n",
    "task2 >> task3\n",
    "task3 >> task1\n",
    "```\n",
    "\n",
    "We should remove last line: task3 >> task1\n",
    "\n",
    "Using the Airflow UI to determine any issues with your DAGs is a great troubleshooting step. For this particular issue, a loop, or cycle, is present within the DAG. Note that technically removing the first dependency would resolve the issue as well, but the comments specifically reference the desired effect. Commenting the desired effect in this way can often help resolve bugs in Airflow DAG execution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using the PythonOperator\n",
    "You've implemented several Airflow tasks using the BashOperator but realize that a couple of specific tasks would be better implemented using Python. You'll implement a task to download and save a file to the system within Airflow."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from requests import get\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Define the method\n",
    "def pull_file(URL, savepath):\n",
    "    r = get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "# Instantiate the DAG object\n",
    "process_sales_dag = DAG('process_sales', default_args=default_args)\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=process_sales_dag\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## More PythonOperators\n",
    "To continue implementing your workflow, you need to add another step to parse and save the changes of the downloaded file. The DAG process_sales_dag is defined and has the pull_file task already added. In this case, the Python function is already defined for you, parse_file(inputfile, outputfile).\n",
    "\n",
    "Note that often when implementing Airflow tasks, you won't necessarily understand the individual steps given to you. As long as you understand how to wrap the steps within Airflow's structure, you'll be able to implement a desired workflow."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from json import load, dump\n",
    "\n",
    "def parse_file(inputfile, outputfile):\n",
    "    with open(inputfile) as infile:\n",
    "      data=load(infile)\n",
    "      with open(outputfile, 'w') as outfile:\n",
    "        dump(data, outfile)\n",
    "\n",
    "# Add another Python task\n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",
    "    # Add the DAG\n",
    "    dag=process_sales_dag\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EmailOperator and dependencies\n",
    "Now that you've successfully defined the PythonOperators for your workflow, your manager would like to receive a copy of the parsed JSON file via email when the workflow completes. The previous tasks are still defined and the DAG process_sales_dag is configured."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Task(EmailOperator): email_manager>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Operator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You've implemented a multi-step workflow using PythonOperators and an EmailOperator while also setting your task dependencies. This type of development is common when implementing an Airflow workflow."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Schedule a DAG via Python\n",
    "You've learned quite a bit about creating DAGs, but now you would like to schedule a specific DAG on a specific day of the week at a certain time. You'd like the code include this information in case a colleague needs to reinstall the DAG to a different server.\n",
    "\n",
    "- Set the start date of the DAG to November 1, 2019.\n",
    "- Configure the retry_delay to 20 minutes. You will learn more about the timedelta object in Chapter 3. For now, you just need to know it expects an integer value.\n",
    "- Use the cron syntax to configure a schedule of every Wednesday at 12:30pm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2019, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You've just scheduled the DAG to run based on your code so there's no ambiguity of what should happen. Of course, it can be modified later via the UI, but the default is behaving as expected."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deciphering Airflow schedules\n",
    "Given the various options for Airflow's schedule_interval, you'd like to verify that you understand exactly how intervals relate to each other, whether it's a cron format, timedelta object, or a preset.\n",
    "\n",
    "![Ordering](images/chap2_1.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Troubleshooting DAG runs\n",
    "You've scheduled a DAG called process_sales which is set to run on the first day of the month and email your manager a copy of the report generated in the workflow. The start_date for the DAG is set to February 15, 2020. Unfortunately it's now March 2nd and your manager did not receive the report and would like to know what happened.\n",
    "\n",
    "Use the information you've learned about Airflow scheduling to determine what the issue is.\n",
    "\n",
    "### Answer: The schedule_interval has not yet passed since the start_date.\n",
    "\n",
    "Scheduling in Airflow can sometimes be more tricky than it first appears. Understanding where to troubleshoot can limit frustration with workflow scheduling.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}